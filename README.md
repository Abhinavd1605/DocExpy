# DocExpy - Document-based Question Answering System

A powerful Generative AI application that allows users to upload documents (PDFs, Word files) and ask intelligent questions based on the content using LLMs and vector search. Now powered by **Groq** for ultra-fast inference!

## ğŸš€ Features

- **ğŸ“ File Upload**: Support for PDF and Word documents (.pdf, .docx, .doc)
- **ğŸ”§ Intelligent Processing**: Automatic text extraction and chunking
- **ğŸ§  Vector Embeddings**: Uses Sentence Transformers for semantic understanding
- **ğŸ’¬ Context-Aware Q&A**: Groq-powered question answering with document context
- **ğŸ“Š Vector Search**: ChromaDB for efficient similarity search
- **ğŸ¨ Professional UI**: Clean, modern Streamlit interface
- **ğŸ“œ Chat History**: Track previous questions and answers
- **ğŸ” Source Attribution**: Shows relevant document chunks used for answers
- **âš¡ Ultra-Fast**: Groq's lightning-fast inference speeds

## ğŸ› ï¸ Tech Stack

- **Frontend**: Streamlit
- **LLM Framework**: LangChain
- **Language Model**: Groq API (Llama 3, Mixtral, Gemma models)
- **Embeddings**: Sentence Transformers (all-MiniLM-L6-v2)
- **Vector Database**: ChromaDB
- **Document Processing**: PyPDF2, python-docx

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- Groq API key (free at console.groq.com)

## ğŸ”§ Installation

1. **Clone or download the project**:
   ```bash
   cd DocExpy
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   - Copy `env_example.txt` content to create `.env` file
   - Add your Groq API key:
   ```bash
   # Create .env file with:
   GROQ_API_KEY=your_groq_api_key_here
   ```

## ğŸš€ Running the Application

1. **Start the Streamlit app**:
   ```bash
   streamlit run app.py
   ```

2. **Open your browser** and navigate to `http://localhost:8501`

## ğŸ“– How to Use

### 1. Upload Documents
- Go to the "ğŸ“ Upload Documents" tab
- Select one or more PDF or Word documents
- Click "ğŸš€ Process Documents" to extract and embed the content

### 2. Ask Questions
- Switch to the "ğŸ’¬ Ask Questions" tab
- Type your question about the uploaded documents
- Click "ğŸ” Search" to get AI-powered answers
- View source context to see which document sections were used

### 3. Review History
- Check the "ğŸ“œ History" tab to see previous questions and answers

## âš™ï¸ Configuration

The application can be configured through the `config.py` file:

- **CHUNK_SIZE**: Size of text chunks for processing (default: 1000)
- **CHUNK_OVERLAP**: Overlap between chunks (default: 200)
- **EMBEDDING_MODEL**: Sentence transformer model (default: all-MiniLM-L6-v2)
- **LLM_MODEL**: Groq model for Q&A (default: llama3-8b-8192)

## ğŸ¤– Available Groq Models

- **llama3-8b-8192**: Fastest, good quality (default)
- **llama3-70b-4096**: Higher quality, slower
- **mixtral-8x7b-32768**: Balanced performance
- **gemma-7b-it**: Google's Gemma model

## ğŸ“ Project Structure

```
DocExpy/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ config.py             # Configuration management
â”œâ”€â”€ document_processor.py  # Document processing utilities
â”œâ”€â”€ vector_store.py       # Vector database management
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ env_example.txt       # Environment variables template
â”œâ”€â”€ README.md            # This file
â””â”€â”€ chroma_db/           # ChromaDB storage (created automatically)
```

## ğŸ”’ Security Notes

- Never commit your `.env` file with real API keys
- Keep your Groq API key secure
- The vector database is stored locally in the `chroma_db/` directory

## ğŸ› Troubleshooting

### Common Issues:

1. **"GROQ_API_KEY environment variable is required"**
   - Make sure you've created a `.env` file with your API key

2. **File upload errors**
   - Ensure your PDF/Word files are not corrupted
   - Check file size limits (10MB max by default)

3. **Memory issues with large documents**
   - Try reducing CHUNK_SIZE in config.py
   - Process fewer documents at once

4. **Slow embedding generation**
   - First time will be slower as the model downloads
   - Subsequent runs will be much faster

## ğŸŒŸ Why Groq?

- **Ultra-fast inference**: 300+ tokens/second
- **Cost-effective**: Free tier with generous limits
- **Multiple models**: Support for Llama 3, Mixtral, and Gemma
- **High quality**: State-of-the-art model performance
- **Easy integration**: Simple API similar to OpenAI

## ğŸ¤ Contributing

Feel free to submit issues, feature requests, or pull requests to improve the application.

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- Groq for ultra-fast LLM inference
- Sentence Transformers for embeddings
- LangChain for the AI framework
- ChromaDB for vector storage
- Streamlit for the web interface